{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "desperate-franchise",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'google.colab' in str(get_ipython()):\n",
    "    !pip install pytorch_lightning\n",
    "    !pip install neuralforecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "chronic-patio",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "import neuralforecast as nf\n",
    "from neuralforecast.data.datasets.epf import EPF\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from neuralforecast.data.tsloader import TimeSeriesLoader\n",
    "from neuralforecast.experiments.utils import create_datasets\n",
    "from neuralforecast.data.tsdataset import IterateWindowsDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "disciplinary-parking",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\") # Comment out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "genuine-tumor",
   "metadata": {},
   "outputs": [],
   "source": [
    "VAL_PERC = .1\n",
    "TEST_PERC = .1\n",
    "first_n=1000 #Grab the first n time-series\n",
    "N_TIME_SERIES = first_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "expected-sphere",
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_model = {}\n",
    "\n",
    "mc_model['seq_len'] = 96 # Input sequence size.\n",
    "mc_model['label_len'] = 96 // 2 # Label sequence size. (Input buffer length for decoder)\n",
    "mc_model['pred_len'] = 96 # Prediction sequence size.\n",
    "mc_model['output_attention'] = False # If true use output attention for Transformer model.\n",
    "mc_model['enc_in'] = N_TIME_SERIES #  Number of encoders in data embedding layers.\n",
    "mc_model['dec_in'] = N_TIME_SERIES #  Number of decoders in data embedding layers.\n",
    "mc_model['d_model'] = 512 #  Number of nodes for embedding layers.\n",
    "mc_model['c_out'] = N_TIME_SERIES # Number of output nodes in projection layer.\n",
    "mc_model['embed'] = 'timeF' #  Type of embedding layers.\n",
    "mc_model['freq'] = 'h' # Frequency for embedding layers.\n",
    "mc_model['dropout'] = 0.05 # Float between (0, 1). Dropout for Transformer.\n",
    "mc_model['factor'] = 1 # Factor for attention layer.\n",
    "mc_model['n_heads'] = 8 #  Number of heads in attention layer.\n",
    "mc_model['d_ff'] = 2_048 #  Number of inputs in encoder layers.\n",
    "mc_model['moving_avg'] = 25  #  Moving average for encoder and decoder layers.\n",
    "mc_model['activation'] = 'gelu' #  Activation function for encoder layer.\n",
    "mc_model['e_layers'] = 2 # Number of encoder layers.\n",
    "mc_model['d_layers'] = 1 # Number of decoder layers.\n",
    "mc_model['loss_train'] = 'MAE' # Loss to optimize. An item from ['MAPE', 'MASE', 'SMAPE', 'MSE', 'MAE', 'QUANTILE', 'QUANTILE2']. \n",
    "mc_model['loss_hypar'] = 0.5 # Hyperparameter for chosen loss.\n",
    "mc_model['loss_valid'] = 'MAE'# Validation loss.An item from ['MAPE', 'MASE', 'SMAPE', 'RMSE', 'MAE', 'QUANTILE'].\n",
    "mc_model['learning_rate'] = 0.001 # Learning rate between (0, 1).\n",
    "mc_model['lr_decay'] = 0.5 # Decreasing multiplier for the learning rate.\n",
    "mc_model['weight_decay'] = 0. # L2 penalty for optimizer.\n",
    "mc_model['lr_decay_step_size'] = 2 # Steps between each learning rate decay.\n",
    "mc_model['random_seed'] = 1 # random_seed for pseudo random pytorch initializer and numpy random generator.\n",
    "\n",
    "# Dataset parameters\n",
    "mc_data = {}\n",
    "mc_data['mode'] = 'iterate_windows'\n",
    "mc_data['n_time_in'] = mc_model['seq_len'] # Input sequence length\n",
    "mc_data['n_time_out'] = mc_model['pred_len'] # Prediction sequence length\n",
    "mc_data['batch_size'] = 1 # Batch size \n",
    "mc_data['normalizer_y'] = None \n",
    "mc_data['normalizer_x'] = None\n",
    "mc_data['max_epochs'] = 1 # Maximum number of training epochs\n",
    "mc_data['max_steps'] = None # maximum number of training steps\n",
    "mc_data['early_stop_patience'] = 20 #Number of consecutive violations of early stopping criteria to end training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "plastic-tobago",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>dept_id</th>\n",
       "      <th>cat_id</th>\n",
       "      <th>store_id</th>\n",
       "      <th>state_id</th>\n",
       "      <th>d</th>\n",
       "      <th>sold</th>\n",
       "      <th>date</th>\n",
       "      <th>wm_yr_wk</th>\n",
       "      <th>...</th>\n",
       "      <th>cat_sold_avg</th>\n",
       "      <th>dept_sold_avg</th>\n",
       "      <th>cat_dept_sold_avg</th>\n",
       "      <th>store_item_sold_avg</th>\n",
       "      <th>cat_item_sold_avg</th>\n",
       "      <th>dept_item_sold_avg</th>\n",
       "      <th>state_store_sold_avg</th>\n",
       "      <th>state_store_cat_sold_avg</th>\n",
       "      <th>store_cat_dept_sold_avg</th>\n",
       "      <th>rolling_sold_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1067150</th>\n",
       "      <td>14370</td>\n",
       "      <td>1437</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>2011-03-05</td>\n",
       "      <td>11106</td>\n",
       "      <td>...</td>\n",
       "      <td>0.561035</td>\n",
       "      <td>0.695801</td>\n",
       "      <td>0.695801</td>\n",
       "      <td>0.321533</td>\n",
       "      <td>0.216553</td>\n",
       "      <td>0.216553</td>\n",
       "      <td>1.304688</td>\n",
       "      <td>0.801758</td>\n",
       "      <td>1.020508</td>\n",
       "      <td>1.142578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1067151</th>\n",
       "      <td>14380</td>\n",
       "      <td>1438</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>2011-03-05</td>\n",
       "      <td>11106</td>\n",
       "      <td>...</td>\n",
       "      <td>0.561035</td>\n",
       "      <td>0.695801</td>\n",
       "      <td>0.695801</td>\n",
       "      <td>0.253906</td>\n",
       "      <td>0.259766</td>\n",
       "      <td>0.259766</td>\n",
       "      <td>1.304688</td>\n",
       "      <td>0.801758</td>\n",
       "      <td>1.020508</td>\n",
       "      <td>1.142578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1067152</th>\n",
       "      <td>14390</td>\n",
       "      <td>1439</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>2011-03-05</td>\n",
       "      <td>11106</td>\n",
       "      <td>...</td>\n",
       "      <td>0.561035</td>\n",
       "      <td>0.695801</td>\n",
       "      <td>0.695801</td>\n",
       "      <td>0.156982</td>\n",
       "      <td>0.076660</td>\n",
       "      <td>0.076660</td>\n",
       "      <td>1.304688</td>\n",
       "      <td>0.801758</td>\n",
       "      <td>1.020508</td>\n",
       "      <td>0.428467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1067153</th>\n",
       "      <td>14400</td>\n",
       "      <td>1440</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>2011-03-05</td>\n",
       "      <td>11106</td>\n",
       "      <td>...</td>\n",
       "      <td>0.561035</td>\n",
       "      <td>0.695801</td>\n",
       "      <td>0.695801</td>\n",
       "      <td>1.694336</td>\n",
       "      <td>2.011719</td>\n",
       "      <td>2.011719</td>\n",
       "      <td>1.304688</td>\n",
       "      <td>0.801758</td>\n",
       "      <td>1.020508</td>\n",
       "      <td>0.285645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1067154</th>\n",
       "      <td>14410</td>\n",
       "      <td>1441</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>2011-03-05</td>\n",
       "      <td>11106</td>\n",
       "      <td>...</td>\n",
       "      <td>0.561035</td>\n",
       "      <td>0.695801</td>\n",
       "      <td>0.695801</td>\n",
       "      <td>0.958984</td>\n",
       "      <td>0.755371</td>\n",
       "      <td>0.755371</td>\n",
       "      <td>1.304688</td>\n",
       "      <td>0.801758</td>\n",
       "      <td>1.020508</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id  item_id  dept_id  cat_id  store_id  state_id   d  sold  \\\n",
       "1067150  14370     1437        3       1         0         0  36     0   \n",
       "1067151  14380     1438        3       1         0         0  36     0   \n",
       "1067152  14390     1439        3       1         0         0  36     0   \n",
       "1067153  14400     1440        3       1         0         0  36     0   \n",
       "1067154  14410     1441        3       1         0         0  36     0   \n",
       "\n",
       "              date  wm_yr_wk  ...  cat_sold_avg  dept_sold_avg  \\\n",
       "1067150 2011-03-05     11106  ...      0.561035       0.695801   \n",
       "1067151 2011-03-05     11106  ...      0.561035       0.695801   \n",
       "1067152 2011-03-05     11106  ...      0.561035       0.695801   \n",
       "1067153 2011-03-05     11106  ...      0.561035       0.695801   \n",
       "1067154 2011-03-05     11106  ...      0.561035       0.695801   \n",
       "\n",
       "         cat_dept_sold_avg  store_item_sold_avg  cat_item_sold_avg  \\\n",
       "1067150           0.695801             0.321533           0.216553   \n",
       "1067151           0.695801             0.253906           0.259766   \n",
       "1067152           0.695801             0.156982           0.076660   \n",
       "1067153           0.695801             1.694336           2.011719   \n",
       "1067154           0.695801             0.958984           0.755371   \n",
       "\n",
       "         dept_item_sold_avg  state_store_sold_avg  state_store_cat_sold_avg  \\\n",
       "1067150            0.216553              1.304688                  0.801758   \n",
       "1067151            0.259766              1.304688                  0.801758   \n",
       "1067152            0.076660              1.304688                  0.801758   \n",
       "1067153            2.011719              1.304688                  0.801758   \n",
       "1067154            0.755371              1.304688                  0.801758   \n",
       "\n",
       "         store_cat_dept_sold_avg  rolling_sold_mean  \n",
       "1067150                 1.020508           1.142578  \n",
       "1067151                 1.020508           1.142578  \n",
       "1067152                 1.020508           0.428467  \n",
       "1067153                 1.020508           0.285645  \n",
       "1067154                 1.020508           0.000000  \n",
       "\n",
       "[5 rows x 42 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_n=1000\n",
    "df=pd.read_pickle(\"m5_aggregate_df.pkl\")\n",
    "df=df.loc[df['id'].isin(df['id'].unique()[0:first_n])]\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "surgical-thirty",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', 'd',\n",
       "       'sold', 'date', 'wm_yr_wk', 'weekday', 'wday', 'month', 'year',\n",
       "       'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2',\n",
       "       'snap_CA', 'snap_TX', 'snap_WI', 'sell_price', 'sold_lag_1',\n",
       "       'sold_lag_2', 'sold_lag_3', 'sold_lag_6', 'sold_lag_12', 'sold_lag_24',\n",
       "       'sold_lag_36', 'iteam_sold_avg', 'state_sold_avg', 'store_sold_avg',\n",
       "       'cat_sold_avg', 'dept_sold_avg', 'cat_dept_sold_avg',\n",
       "       'store_item_sold_avg', 'cat_item_sold_avg', 'dept_item_sold_avg',\n",
       "       'state_store_sold_avg', 'state_store_cat_sold_avg',\n",
       "       'store_cat_dept_sold_avg', 'rolling_sold_mean'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "finished-louis",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Static variables\n",
    "S_df = None\n",
    "\n",
    "#Training Data\n",
    "Y_df=df[['id','date','sold']]\n",
    "Y_df.columns=['unique_id','ds','y']\n",
    "\n",
    "#Exogenous Variable Training Data\n",
    "X_df=df[['id','date','sold_lag_2', 'sold_lag_3', 'sold_lag_6', 'sold_lag_12']]\n",
    "X_df.columns=['unique_id','ds','ex_1','ex_2','ex_3','ex_4']\n",
    "\n",
    "f_cols = X_df.drop(columns=['unique_id', 'ds']).columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "functional-jewelry",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Train Validation splits\n",
      "\n",
      "INFO:root:                    ds           \n",
      "                   min        max\n",
      "sample_mask                      \n",
      "0           2015-05-31 2016-06-19\n",
      "1           2011-03-05 2015-05-30\n",
      "INFO:root:\n",
      "Total data \t\t\t1934000 time stamps \n",
      "Available percentage=100.0, \t1934000 time stamps \n",
      "Insample  percentage=80.04, \t1548000 time stamps \n",
      "Outsample percentage=19.96, \t386000 time stamps \n",
      "\n",
      "INFO:root:Train Validation splits\n",
      "\n",
      "INFO:root:                    ds           \n",
      "                   min        max\n",
      "sample_mask                      \n",
      "0           2011-03-05 2016-06-19\n",
      "1           2015-05-31 2015-12-09\n",
      "INFO:root:\n",
      "Total data \t\t\t1934000 time stamps \n",
      "Available percentage=100.0, \t1934000 time stamps \n",
      "Insample  percentage=9.98, \t193000 time stamps \n",
      "Outsample percentage=90.02, \t1741000 time stamps \n",
      "\n",
      "INFO:root:Train Validation splits\n",
      "\n",
      "INFO:root:                    ds           \n",
      "                   min        max\n",
      "sample_mask                      \n",
      "0           2011-03-05 2015-12-09\n",
      "1           2015-12-10 2016-06-19\n",
      "INFO:root:\n",
      "Total data \t\t\t1934000 time stamps \n",
      "Available percentage=100.0, \t1934000 time stamps \n",
      "Insample  percentage=9.98, \t193000 time stamps \n",
      "Outsample percentage=90.02, \t1741000 time stamps \n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_ds = Y_df[\"ds\"].nunique()\n",
    "n_val = int(VAL_PERC * n_ds)\n",
    "n_test = int(TEST_PERC * n_ds)\n",
    "\n",
    "train_dataset, val_dataset, test_dataset, scaler_y = create_datasets(mc=mc_data,\n",
    "                                                                     S_df=None, \n",
    "                                                                     Y_df=Y_df, X_df=X_df,\n",
    "                                                                     f_cols=f_cols,\n",
    "                                                                     ds_in_val=n_val,\n",
    "                                                                     ds_in_test=n_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "generous-fight",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=int(mc_data['batch_size']),\n",
    "                          shuffle=True,\n",
    "                          drop_last=True)\n",
    "\n",
    "val_loader = DataLoader(dataset=val_dataset,\n",
    "                        batch_size=int(mc_data['batch_size']),\n",
    "                        shuffle=False)\n",
    "\n",
    "test_loader = DataLoader(dataset=test_dataset,\n",
    "                         batch_size=int(mc_data['batch_size']),\n",
    "                         shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "expanded-bunny",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nf.models.transformer.autoformer.Autoformer(**mc_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "honey-sunrise",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "Set SLURM handle signals.\n",
      "\n",
      "  | Name  | Type        | Params\n",
      "--------------------------------------\n",
      "0 | model | _Autoformer | 15.6 M\n",
      "--------------------------------------\n",
      "15.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "15.6 M    Total params\n",
      "62.484    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0834eeb083ea4cdcbe8dbef7280143cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25f1601862f24c6bb96a1630109c649a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "early_stopping = pl.callbacks.EarlyStopping(monitor='val_loss', \n",
    "                                            min_delta=1e-4, \n",
    "                                            patience=mc_data['early_stop_patience'],\n",
    "                                            verbose=False,\n",
    "                                            mode=\"min\")\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=mc_data['max_epochs'], \n",
    "                     max_steps=mc_data['max_steps'],\n",
    "                     gradient_clip_val=1.0,\n",
    "                     progress_bar_refresh_rate=10, \n",
    "                     check_val_every_n_epoch=1,\n",
    "                     log_every_n_steps=500, \n",
    "                     callbacks=[early_stopping])\n",
    "\n",
    "trainer.fit(model, train_loader, val_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complimentary-pride",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expected-floor",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "desperate-witness",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complicated-beach",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "forecasting",
   "language": "python",
   "name": "forecasting"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
