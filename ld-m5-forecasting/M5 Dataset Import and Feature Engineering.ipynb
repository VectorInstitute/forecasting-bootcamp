{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "comparable-relative",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import ceil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlike-tennis",
   "metadata": {},
   "source": [
    "Import data and reduce memory usage. Taken from notebook [here](https://github.com/matthiasanderer/m5-accuracy-competition/blob/main/m5-simple-fe-evaluation.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dramatic-practice",
   "metadata": {},
   "source": [
    "Global variables and helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "lesser-convergence",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET = 'sales'          # Our main target\n",
    "END_TRAIN = 1913 + 28     # Last day in train set\n",
    "MAIN_INDEX = ['id', 'd']  # We can identify item by these columns\n",
    "ROOT_PATH = '/ssd003/projects/forecasting_bootcamp/bootcamp_datasets/m5-forecasting-accuracy'\n",
    "\n",
    "def reduce_mem_usage(df):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64'] \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                       df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "    return df\n",
    "\n",
    "def merge_by_concat(df1, df2, merge_on):\n",
    "    merged_gf = df1[merge_on]\n",
    "    merged_gf = merged_gf.merge(df2, on=merge_on, how='left')\n",
    "    new_columns = [col for col in list(merged_gf) if col not in merge_on]\n",
    "    df1 = pd.concat([df1, merged_gf[new_columns]], axis=1)\n",
    "    return df1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "standard-illness",
   "metadata": {},
   "source": [
    "Dataframes that uses less memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "laden-galaxy",
   "metadata": {},
   "outputs": [],
   "source": [
    "calendar_df = pd.read_csv(os.path.join(ROOT_PATH, 'calendar.csv'))\n",
    "calendar_df = reduce_mem_usage(calendar_df)\n",
    "\n",
    "prices_df = pd.read_csv(os.path.join(ROOT_PATH, 'sell_prices.csv'))\n",
    "prices_df = reduce_mem_usage(prices_df)\n",
    "\n",
    "train_df = pd.read_csv(os.path.join(ROOT_PATH, 'sales_train_evaluation.csv'))\n",
    "train_df = train_df.drop(['d_{}'.format(str(i)) for i in range(1, 29)], axis=1)\n",
    "train_df = reduce_mem_usage(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "female-booking",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tranform horizontal representation to vertical \"view\"\n",
    "# Our \"index\" will be 'id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'\n",
    "# and labels are 'd_' columns\n",
    "\n",
    "index_columns = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']\n",
    "\n",
    "grid_df = pd.melt(train_df, \n",
    "                  id_vars = index_columns, \n",
    "                  var_name = 'd', \n",
    "                  value_name = TARGET)\n",
    "\n",
    "# Add \"test set\" to our grid to make predictions\n",
    "add_grid = pd.DataFrame()\n",
    "for i in range(1, 29):\n",
    "    temp_df = train_df[index_columns]\n",
    "    temp_df = temp_df.drop_duplicates()\n",
    "    temp_df['d'] = 'd_'+ str(END_TRAIN+i)\n",
    "    temp_df[TARGET] = np.nan\n",
    "    add_grid = pd.concat([add_grid, temp_df])\n",
    "\n",
    "grid_df = pd.concat([grid_df, add_grid])\n",
    "grid_df = grid_df.reset_index(drop=True)\n",
    "\n",
    "# Remove some temporary dfs and original train_df\n",
    "del temp_df, add_grid, train_df\n",
    "\n",
    "# Free some memory by converting \"strings\" to categorical\n",
    "for col in index_columns:\n",
    "    grid_df[col] = grid_df[col].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eligible-satisfaction",
   "metadata": {},
   "source": [
    "Add features: product release date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tracked-muscle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prices are set by week so it will not have very accurate release week \n",
    "release_df = prices_df.groupby(['store_id', 'item_id'])['wm_yr_wk'].agg(['min']).reset_index()\n",
    "release_df.columns = ['store_id', 'item_id', 'release']\n",
    "\n",
    "# Merge release_df\n",
    "grid_df = merge_by_concat(grid_df, release_df, ['store_id', 'item_id'])\n",
    "del release_df\n",
    "\n",
    "# Remove some \"zeros\" rows from grid_df \n",
    "grid_df = merge_by_concat(grid_df, calendar_df[['wm_yr_wk', 'd']], ['d'])\n",
    "                      \n",
    "# Cut off some rows and save memory \n",
    "grid_df = grid_df[grid_df['wm_yr_wk'] >= grid_df['release']]\n",
    "grid_df = grid_df.reset_index(drop=True)\n",
    "\n",
    "# Minify the release values.\n",
    "grid_df['release'] = grid_df['release'] - grid_df['release'].min()\n",
    "grid_df['release'] = grid_df['release'].astype(np.int16)\n",
    "\n",
    "# Save BASE grid for model training\n",
    "grid_df.to_pickle('grid_part_1.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collective-theorem",
   "metadata": {},
   "source": [
    "Add features: price features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consolidated-father",
   "metadata": {},
   "outputs": [],
   "source": [
    "prices_df['price_max'] = prices_df.groupby(['store_id', 'item_id'])['sell_price'].transform('max')\n",
    "prices_df['price_min'] = prices_df.groupby(['store_id', 'item_id'])['sell_price'].transform('min')\n",
    "prices_df['price_std'] = prices_df.groupby(['store_id', 'item_id'])['sell_price'].transform('std')\n",
    "prices_df['price_mean'] = prices_df.groupby(['store_id', 'item_id'])['sell_price'].transform('mean')\n",
    "prices_df['price_norm'] = prices_df['sell_price'] / prices_df['price_max']\n",
    "prices_df['price_nunique'] = prices_df.groupby(['store_id', 'item_id'])['sell_price'].transform('nunique')\n",
    "prices_df['item_nunique'] = prices_df.groupby(['store_id', 'sell_price'])['item_id'].transform('nunique')\n",
    "\n",
    "calendar_prices = calendar_df[['wm_yr_wk', 'month', 'year']]\n",
    "calendar_prices = calendar_prices.drop_duplicates(subset=['wm_yr_wk'])\n",
    "prices_df = prices_df.merge(calendar_prices[['wm_yr_wk', 'month', 'year']], on=['wm_yr_wk'], how='left')\n",
    "del calendar_prices\n",
    "\n",
    "prices_df['price_momentum'] = prices_df['sell_price'] / prices_df.groupby(['store_id', 'item_id'])['sell_price'].transform(lambda x: x.shift(1))\n",
    "prices_df['price_momentum_m'] = prices_df['sell_price'] / prices_df.groupby(['store_id', 'item_id', 'month'])['sell_price'].transform('mean')\n",
    "prices_df['price_momentum_y'] = prices_df['sell_price'] / prices_df.groupby(['store_id', 'item_id', 'year'])['sell_price'].transform('mean')\n",
    "del prices_df['month'], prices_df['year']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dramatic-capture",
   "metadata": {},
   "source": [
    "Merge prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "respected-chaos",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_columns = list(grid_df)\n",
    "grid_df = grid_df.merge(prices_df, on=['store_id', 'item_id', 'wm_yr_wk'], how='left')\n",
    "keep_columns = [col for col in list(grid_df) if col not in original_columns]\n",
    "grid_df = grid_df[MAIN_INDEX + keep_columns]\n",
    "grid_df = reduce_mem_usage(grid_df)\n",
    "\n",
    "# Safe part 2\n",
    "grid_df.to_pickle('grid_part_2.pkl')\n",
    "\n",
    "# We don't need prices_df anymore\n",
    "del prices_df\n",
    "\n",
    "# Load part 1\n",
    "grid_df = pd.read_pickle('grid_part_1.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smoking-rhythm",
   "metadata": {},
   "source": [
    "Merge calendar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nearby-tennis",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grid_df = grid_df[MAIN_INDEX]\n",
    "\n",
    "# Merge calendar partly\n",
    "icols = ['date',\n",
    "         'd',\n",
    "         'event_name_1',\n",
    "         'event_type_1',\n",
    "         'event_name_2',\n",
    "         'event_type_2',\n",
    "         'snap_CA',\n",
    "         'snap_TX',\n",
    "         'snap_WI']\n",
    "\n",
    "grid_df = grid_df.merge(calendar_df[icols], on=['d'], how='left')\n",
    "\n",
    "# Minify data\n",
    "# 'snap_' columns we can convert to bool or int8\n",
    "icols = ['event_name_1',\n",
    "         'event_type_1',\n",
    "         'event_name_2',\n",
    "         'event_type_2',\n",
    "         'snap_CA',\n",
    "         'snap_TX',\n",
    "         'snap_WI']\n",
    "for col in icols:\n",
    "    grid_df[col] = grid_df[col].astype('category')\n",
    "\n",
    "# Convert to DateTime\n",
    "grid_df['date'] = pd.to_datetime(grid_df['date'])\n",
    "\n",
    "# Make some features from date\n",
    "grid_df['tm_d'] = grid_df['date'].dt.day.astype(np.int8)\n",
    "grid_df['tm_w'] = grid_df['date'].dt.week.astype(np.int8)\n",
    "grid_df['tm_m'] = grid_df['date'].dt.month.astype(np.int8)\n",
    "grid_df['tm_y'] = grid_df['date'].dt.year\n",
    "grid_df['tm_y'] = (grid_df['tm_y'] - grid_df['tm_y'].min()).astype(np.int8)\n",
    "grid_df['tm_wm'] = grid_df['tm_d'].apply(lambda x: ceil(x/7)).astype(np.int8)\n",
    "\n",
    "grid_df['tm_dw'] = grid_df['date'].dt.dayofweek.astype(np.int8)\n",
    "grid_df['tm_w_end'] = (grid_df['tm_dw']>=5).astype(np.int8)\n",
    "\n",
    "# Remove date\n",
    "del grid_df['date']\n",
    "\n",
    "# Save part 3\n",
    "grid_df.to_pickle('grid_part_3.pkl')\n",
    "\n",
    "# We don't need calendar_df anymore\n",
    "del calendar_df, grid_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extreme-pocket",
   "metadata": {},
   "source": [
    "Final grid dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alleged-reaction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'd' to int\n",
    "grid_df = pd.read_pickle('grid_part_1.pkl')\n",
    "grid_df['d'] = grid_df['d'].apply(lambda x: x[2:]).astype(np.int16)\n",
    "\n",
    "# Remove 'wm_yr_wk' as test values are not in train set\n",
    "del grid_df['wm_yr_wk']\n",
    "grid_df.to_pickle('grid_part_1.pkl')\n",
    "\n",
    "del grid_df\n",
    "\n",
    "grid_df = pd.concat([pd.read_pickle('grid_part_1.pkl'),\n",
    "                     pd.read_pickle('grid_part_2.pkl').iloc[:,2:],\n",
    "                     pd.read_pickle('grid_part_3.pkl').iloc[:,2:]],\n",
    "                     axis=1)\n",
    "\n",
    "grid_df.to_pickle('grid.pkl')\n",
    "for i in range(1, 4):\n",
    "    os.remove('grid_part_{}.pkl'.format(str(i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seasonal-southwest",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "forecasting",
   "language": "python",
   "name": "forecasting"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
